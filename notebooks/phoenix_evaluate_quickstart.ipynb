{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoenix Evals Quickstart\n",
    "\n",
    "This quickstart shows how Phoenix helps you evaluate data from your LLM application (e.g., inputs, outputs, retrieved documents).\n",
    "\n",
    "You will:\n",
    "\n",
    "- Export a dataframe from your Phoenix session that contains traces from an instrumented LLM application,\n",
    "- Evaluate your trace data for:\n",
    "  - Relevance: Are the retrieved documents grounded in the response?\n",
    "  - Q&A correctness: Are your application's responses grounded in the retrieved context?\n",
    "  - Hallucinations: Is your application making up false information?\n",
    "- Ingest the evaluations into Phoenix to see the results annotated on the corresponding spans and traces.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "First, install Phoenix with `pip install arize-phoenix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To address temporarily CERTIFICATE_VERIFY_FAILED\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "from phoenix.trace.trace_dataset import TraceDataset\n",
    "from phoenix.trace.utils import json_lines_to_df\n",
    "\n",
    "# To get you up and running quickly, we'll download some pre-existing trace data collected from a LlamaIndex application (in practice, this data would be collected by instrumenting your LLM application with an OpenInference-compatible tracer)\n",
    "traces_url = \"https://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/trace.jsonl\"\n",
    "with urlopen(traces_url) as response:\n",
    "    lines = [line.decode(\"utf-8\") for line in response.readlines()]\n",
    "trace_df = json_lines_to_df(lines)\n",
    "\n",
    "# Constructs a TraceDataset from a dataframe of spans\n",
    "trace_ds = TraceDataset(trace_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace.jsonl sample:\n",
    "```json\n",
    "{\"name\": \"query\", \"context\": {\"trace_id\": \"f40dc5d5-08b7-4e23-80e1-2cd6e9f0cf29\", \"span_id\": \"bce5b9ae-4587-4ead-9ccc-de3fe29257bc\"}, \"span_kind\": \"CHAIN\", \"parent_id\": null, \"start_time\": \"2023-12-11T17:57:17.891021+00:00\", \"end_time\": \"2023-12-11T17:57:20.075141+00:00\", \"status_code\": \"OK\", \"status_message\": \"\", \"attributes\": {\"input.value\": \"How can I query for a monitor's status using GraphQL?\", \"input.mime_type\": \"text/plain\", \"output.value\": \"You can query for a monitor's status using GraphQL by including the \\\"status\\\" field in your query.\", \"output.mime_type\": \"text/plain\"}, \"events\": [], \"conversation\": null}\n",
    "{\"name\": \"synthesize\", \"context\": {\"trace_id\": \"f40dc5d5-08b7-4e23-80e1-2cd6e9f0cf29\", \"span_id\": \"3d59ca9b-5d68-4773-856f-5243cba51647\"}, \"span_kind\": \"CHAIN\", \"parent_id\": \"bce5b9ae-4587-4ead-9ccc-de3fe29257bc\", \"start_time\": \"2023-12-11T17:57:18.973513+00:00\", \"end_time\": \"2023-12-11T17:57:20.075056+00:00\", \"status_code\": \"OK\", \"status_message\": \"\", \"attributes\": {\"input.value\": \"How can I query for a monitor's status using GraphQL?\", \"input.mime_type\": \"text/plain\", \"output.value\": \"You can query for a monitor's status using GraphQL by including the \\\"status\\\" field in your query.\", \"output.mime_type\": \"text/plain\"}, \"events\": [], \"conversation\": null}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix. You can open use Phoenix within your notebook or in a separate browser window by opening the URL.\n",
    "To note this trace data dates back 12/11/2023 around 12:57PM - make sure to select \"All Time\" in the webapp to see it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n",
      "üì∫ Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722087673.990868  674680 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd700a064c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import phoenix as px\n",
    "\n",
    "session = px.launch_app(trace=trace_ds)\n",
    "session.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export your retrieved documents and query data from your session into a pandas dataframe.\n",
    "\n",
    "Note: If you are interested in a different subset of your data, you can export with a custom query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())\n",
    "queries_df = get_qa_with_reference(px.Client())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phoenix evaluates your application data by prompting an LLM to classify whether a retrieved document is relevant or irrelevant to the corresponding query, whether a response is grounded in a retrieved document, etc. You can even get explanations generated by the LLM to help you understand the results of your evaluations!\n",
    "\n",
    "This quickstart uses OpenAI and requires an OpenAI API key, but we support a wide variety of APIs and models.  # TODO: Add link\n",
    "\n",
    "Install the OpenAI SDK with `pip install openai` and instantiate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import OpenAIModel\n",
    "\n",
    "eval_model = OpenAIModel(model = \"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll next define your evaluators. Evaluators are built on top of language models and prompt the LLM to assess the quality of responses, the relevance of retrieved documents, etc., and provide a quality signal even in the absence of human-labeled data. Pick an evaluator type and instantiate it with the language model you want to use to perform evaluations using our battle-tested evaluation templates.\n",
    "\n",
    "![A diagram depicting how evaluators are composed of LLMs and evaluation prompt templates and product labels, scores, and explanations from input data (e.g., queries, references, outputs, etc.)](https://storage.googleapis.com/arize-phoenix-assets/assets/docs/notebooks/evals/evaluators_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    HallucinationEvaluator,\n",
    "    QAEvaluator,\n",
    "    RelevanceEvaluator,\n",
    ")\n",
    "\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722087742.331125  673357 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722087742.331332  673357 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc250b88da484dbfb7a69559925665a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/8 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722087742.431732  673357 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722087742.431907  673357 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d25b1ae8df408bac326c66c2daf904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/8 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from phoenix.evals import (\n",
    "    run_evals,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrency in notebook environments\n",
    "\n",
    "hallucination_eval_df, qa_correctness_eval_df = run_evals(\n",
    "    dataframe=queries_df,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df = run_evals(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log your evaluations to your running Phoenix session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n",
    "    DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your evaluations should now appear as annotations on your spans in Phoenix!\n",
    "You can view aggregate evaluation statistics, surface problematic spans, understand the LLM's reason for each evaluation by reading the corresponding explanation, and pinpoint the cause (irrelevant retrievals, incorrect parameterization of your LLM, etc.) of your LLM application's poor responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî•üê¶ Open back up Phoenix in case you closed it: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "print(f\"üî•üê¶ Open back up Phoenix in case you closed it: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view aggregate evaluation statistics, surface problematic spans, understand the LLM's reason for each evaluation by reading the corresponding explanation, and pinpoint the cause (irrelevant retrievals, incorrect parameterization of your LLM, etc.) of your LLM application's poor responses.\n",
    "\n",
    "![A view of the Phoenix UI with evaluation annotations](https://storage.googleapis.com/arize-phoenix-assets/assets/docs/notebooks/evals/traces_with_evaluation_annotations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
