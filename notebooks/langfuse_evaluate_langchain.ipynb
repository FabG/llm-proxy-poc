{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWL354n0DECo"
   },
   "source": [
    "---\n",
    "description: Cookbook that demonstrates how to run Langchain evaluations on data in Langfuse.\n",
    "category: Evaluation\n",
    "---\n",
    "\n",
    "# Run Langchain Evaluations on data in Langfuse\n",
    "\n",
    "This cookbook shows how model-based evaluations can be used to automate the evaluation of production completions in Langfuse. This example uses Langchain and is adaptable to other libraries. Which library is the best to use depends heavily on the use case.\n",
    "\n",
    "This cookbook follows three steps:\n",
    "1. Fetch production `generations` stored in Langfuse\n",
    "2. Evaluate these `generations` using Langchain\n",
    "3. Ingest results back into Langfuse as `scores`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbfTYaTkEu3G"
   },
   "source": [
    "### Setup\n",
    "\n",
    "First you need to install Langfuse and Langchain via pip and then set the environment variables.  \n",
    "LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY, LANGFUSE_HOST and OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Qclwxd9LRPAL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langfuse langchain langchain-openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CQhmQQpLRa1K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['EVAL_MODEL'] = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "# Langchain Eval types\n",
    "EVAL_TYPES={\n",
    "    \"hallucination\": True,\n",
    "    \"conciseness\": True,\n",
    "    \"relevance\": True,\n",
    "    \"coherence\": True,\n",
    "    \"harmfulness\": True,\n",
    "    \"maliciousness\": True,\n",
    "    \"helpfulness\": True,\n",
    "    \"controversiality\": True,\n",
    "    \"misogyny\": True,\n",
    "    \"criminality\": True,\n",
    "    \"insensitivity\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yiwrz1-mavJ4"
   },
   "source": [
    "Initialize the Langfuse Python SDK, more information [here](https://langfuse.com/docs/sdk/python#1-installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8viV4KT5RMjA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse()\n",
    "langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjMZ1VLhF2Vv"
   },
   "source": [
    "### Fetching data\n",
    "\n",
    "Load all `generations` from Langfuse filtered by `name`, in this case `OpenAI`. Names are used in Langfuse to identify different types of generations within an application. Change it to the name you want to evaluate.\n",
    "\n",
    "Checkout [docs](https://langfuse.com/docs/sdk/python#generation) on how to set the name when ingesting an LLM Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3r3jOEX0RvXi"
   },
   "outputs": [],
   "source": [
    "def fetch_all_pages(name=None, user_id = None, limit=50):\n",
    "    page = 1\n",
    "    all_data = []\n",
    "\n",
    "    while True:\n",
    "        response = langfuse.get_generations(name=name, limit=limit, user_id=user_id, page=page)\n",
    "        if not response.data:\n",
    "            break\n",
    "\n",
    "        all_data.extend(response.data)\n",
    "        page += 1\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cAnLShvjBDBU"
   },
   "outputs": [],
   "source": [
    "generations = fetch_all_pages(name='my_generation_name', user_id='user:abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYM6UG_dGbb6"
   },
   "source": [
    "### Set up evaluation functions\n",
    "\n",
    "In this section, we define functions to set up the Langchain eval based on the entries in `EVAL_TYPES`. Hallucinations require their own function. More on the Langchain evals can be found [here](https://python.langchain.com/docs/guides/evaluation/string/criteria_eval_chain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7NijTmslvyK8"
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain\n",
    "\n",
    "def get_evaluator_for_key(key: str):\n",
    "  llm = OpenAI(temperature=0, model=os.environ.get('EVAL_MODEL'))\n",
    "  return load_evaluator(\"criteria\", criteria=key, llm=llm)\n",
    "\n",
    "def get_hallucination_eval():\n",
    "  criteria = {\n",
    "    \"hallucination\": (\n",
    "      \"Does this submission contain information\"\n",
    "      \" not present in the input or reference?\"\n",
    "    ),\n",
    "  }\n",
    "  llm = OpenAI(temperature=0, model=os.environ.get('EVAL_MODEL'))\n",
    "\n",
    "  return LabeledCriteriaEvalChain.from_llm(\n",
    "      llm=llm,\n",
    "      criteria=criteria,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzZZfztGdrIQ"
   },
   "source": [
    "### Execute evaluation\n",
    "\n",
    "Below, we execute the evaluation for each `Generation` loaded above. Each score is ingested into Langfuse via [`langfuse.score()`](https://langfuse.com/docs/scores).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qMa2OEtqvyGg"
   },
   "outputs": [],
   "source": [
    "def execute_eval_and_score():\n",
    "\n",
    "  for generation in generations:\n",
    "    criteria = [key for key, value in EVAL_TYPES.items() if value and key != \"hallucination\"]\n",
    "\n",
    "    for criterion in criteria:\n",
    "      eval_result = get_evaluator_for_key(criterion).evaluate_strings(\n",
    "          prediction=generation.output,\n",
    "          input=generation.input,\n",
    "      )\n",
    "      print(eval_result)\n",
    "\n",
    "      langfuse.score(\n",
    "          name=criterion, \n",
    "          trace_id=generation.trace_id, \n",
    "          observation_id=generation.id, \n",
    "          value=eval_result[\"score\"], \n",
    "          comment=eval_result['reasoning'])\n",
    "\n",
    "execute_eval_and_score()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YcTF-z8eeL0a"
   },
   "outputs": [],
   "source": [
    "# hallucination\n",
    "def eval_hallucination():\n",
    "\n",
    "  chain = get_hallucination_eval()\n",
    "\n",
    "  for generation in generations:\n",
    "    eval_result = chain.evaluate_strings(\n",
    "      prediction=generation.output,\n",
    "      input=generation.input,\n",
    "      reference=generation.input\n",
    "    )\n",
    "    print(eval_result)\n",
    "    if eval_result is not None and eval_result[\"score\"] is not None and eval_result[\"reasoning\"] is not None:\n",
    "      langfuse.score(name='hallucination', trace_id=generation.trace_id, observation_id=generation.id, value=eval_result[\"score\"], comment=eval_result['reasoning'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "n4zeFEKlfjQ-"
   },
   "outputs": [],
   "source": [
    "if EVAL_TYPES.get(\"hallucination\") == True:\n",
    "  eval_hallucination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-ROOd8d8rdl6"
   },
   "outputs": [],
   "source": [
    "# SDK is async, make sure to await all requests\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsKpVyYdavJ5"
   },
   "source": [
    "### See Scores in Langfuse\n",
    "\n",
    " In the Langfuse UI, you can filter Traces by `Scores` and look into the details for each. Check out Langfuse Analytics to understand the impact of new prompt versions or application releases on these scores.\n",
    "\n",
    "![Image of Trace](https://langfuse.com/images/docs/trace-conciseness-score.jpg)\n",
    "_Example trace with conciseness score_\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
