{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIDdGw6hepiF"
   },
   "source": [
    "---\n",
    "description: This notebook demonstrates how to run UpTrain's evaluation metrics on the traces generated by Langfuse.\n",
    "category: Evaluation\n",
    "---\n",
    "\n",
    "# Evaluate Langfuse LLM Traces with UpTrain\n",
    "\n",
    "UpTrain's open-source library offers a series of evaluation metrics to assess LLM applications.\n",
    "\n",
    "This notebook demonstrates how to run UpTrain's evaluation metrics on the traces generated by Langfuse. In Langfuse you can then monitor these scores over time or use them to compare different experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You can get your Langfuse API keys [here](https://cloud.langfuse.com/) and OpenAI API key [here](https://platform.openai.com/api-keys)\n",
    "LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY, LANGFUSE_HOST and OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langfuse datasets uptrain litellm openai eval_type_backport --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lfVmRQlepiI"
   },
   "source": [
    "## Sample Dataset\n",
    "\n",
    "We use this dataset to represent [traces](https://langfuse.com/docs/tracing) that you have logged to Langfuse. In a production environment, you would use your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-wEyOQzepiI",
    "outputId": "48b3948c-22f1-4802-d5e7-2f3c6eb9689d"
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"question\": \"What are the symptoms of a heart attack?\",\n",
    "        \"context\": \"A heart attack, or myocardial infarction, occurs when the blood supply to the heart muscle is blocked. Chest pain is a good symptom of heart attack, though there are many others.\",\n",
    "        \"response\": \"Symptoms of a heart attack may include chest pain or discomfort, shortness of breath, nausea, lightheadedness, and pain or discomfort in one or both arms, the jaw, neck, or back.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can stress cause physical health problems?\",\n",
    "        \"context\": \"Stress is the body's response to challenges or threats. Yes, chronic stress can contribute to various physical health problems, including cardiovascular issues.\",\n",
    "        \"response\": \"Yes, chronic stress can contribute to various physical health problems, including cardiovascular issues, and a weakened immune system.\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"What are the symptoms of a heart attack?\",\n",
    "        'context': \"A heart attack, or myocardial infarction, occurs when the blood supply to the heart muscle is blocked. Symptoms of a heart attack may include chest pain or discomfort, shortness of breath and nausea.\",\n",
    "        'response': \"Heart attack symptoms are usually just indigestion and can be relieved with antacids.\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"Can stress cause physical health problems?\",\n",
    "        'context': \"Stress is the body's response to challenges or threats. Yes, chronic stress can contribute to various physical health problems, including cardiovascular issues.\",\n",
    "        'response': \"Stress is not real, it is just imaginary!\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations using UpTrain\n",
    "\n",
    "We have used the following 3 metrics from UpTrain's open-source library:\n",
    "\n",
    "1. [Context Relevance](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance): Evaluates how relevant the retrieved context is to the question specified.\n",
    "\n",
    "2. [Factual Accuracy](https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy): Evaluates whether the response generated is factually correct and grounded by the provided context.\n",
    "\n",
    "3. [Response Completeness](https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness): Evaluates whether the response has answered all the aspects of the question specified.\n",
    "\n",
    "You can look at the complete list of UpTrain's supported metrics [here](https://docs.uptrain.ai/predefined-evaluations/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muptrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvalLLM, Evals\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.venv/llm-proxy/lib/python3.8/site-packages/lazy_loader/__init__.py:82\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[1;32m     81\u001b[0m     submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 82\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/llm-proxy/lib/python3.8/site-packages/uptrain/framework/evalllm.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muptrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnOp\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muptrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_prompt, check_openai_api_key\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muptrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremote\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m APIClientWithoutAuth, DataSchema\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muptrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings\n",
      "File \u001b[0;32m~/.venv/llm-proxy/lib/python3.8/site-packages/uptrain/utilities/utils.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimplementations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdirfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DirFileSystem\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muptrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Evals,\n\u001b[1;32m     10\u001b[0m     ResponseMatching,\n\u001b[1;32m     11\u001b[0m     GuidelineAdherence,\n\u001b[1;32m     12\u001b[0m     ConversationSatisfaction,\n\u001b[1;32m     13\u001b[0m     JailbreakDetection,\n\u001b[1;32m     14\u001b[0m     CritiqueTone,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muptrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lazy_load_dep\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_openai_api_key\u001b[39m(api_key):\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.venv/llm-proxy/lib/python3.8/site-packages/lazy_loader/__init__.py:82\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[1;32m     81\u001b[0m     submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 82\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/llm-proxy/lib/python3.8/site-packages/uptrain/framework/evals.py:63\u001b[0m\n\u001b[1;32m     59\u001b[0m     user_persona: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m     llm_persona: t\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomPromptEval\u001b[39;00m(ParametricEval):\n\u001b[1;32m     64\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m  \u001b[38;5;66;03m# Evaluation prompt for the LLM\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     choices: \u001b[38;5;28mlist\u001b[39m[\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m     67\u001b[0m     ]  \u001b[38;5;66;03m# We only support Grading evals, list of choices for the LLM to select from. ex: [Correct, Incorrect]\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/llm-proxy/lib/python3.8/site-packages/uptrain/framework/evals.py:65\u001b[0m, in \u001b[0;36mCustomPromptEval\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomPromptEval\u001b[39;00m(ParametricEval):\n\u001b[1;32m     64\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m  \u001b[38;5;66;03m# Evaluation prompt for the LLM\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     choices: \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# We only support Grading evals, list of choices for the LLM to select from. ex: [Correct, Incorrect]\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     choice_scores: t\u001b[38;5;241m.\u001b[39mUnion[\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]\n\u001b[1;32m     70\u001b[0m     ]  \u001b[38;5;66;03m# Scores associated for each choice. ex: [1.0, 0.0]\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     eval_type: t\u001b[38;5;241m.\u001b[39mLiteral[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcot_classify\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcot_classify\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from uptrain import EvalLLM, Evals\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "eval_llm = EvalLLM(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "res = eval_llm.evaluate(\n",
    "    data = data,\n",
    "    checks = [Evals.CONTEXT_RELEVANCE, Evals.FACTUAL_ACCURACY, Evals.RESPONSE_COMPLETENESS]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Langfuse\n",
    "\n",
    "There are two main ways to run evaluations:\n",
    "\n",
    "1. Score each Trace (in development): This means you will run the UpTrain evaluations for each trace item.\n",
    "\n",
    "2. Score in Batches (in production): In this method we will simulate fetching production traces on a periodic basis to score them using the UpTrain evaluators. Often, you'll want to sample the traces instead of scoring all of them to control evaluation costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development: Score each trace while it's created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    " \n",
    "langfuse = Langfuse()\n",
    "\n",
    "langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mock the instrumentation of your application by using the sample dataset. See the [quickstart](https://langfuse.com/docs/get-started) to integrate Langfuse with your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new trace when you get a question\n",
    "question = data[0]['question']\n",
    "trace = langfuse.trace(name = \"uptrain trace\")\n",
    "\n",
    "# retrieve the relevant chunks\n",
    "# chunks = get_similar_chunks(question)\n",
    "context = data[0]['context']\n",
    "# pass it as span\n",
    "trace.span(\n",
    "    name = \"retrieval\", input={'question': question}, output={'context': context}\n",
    ")\n",
    "\n",
    "# use llm to generate a answer with the chunks\n",
    "# answer = get_response_from_llm(question, chunks)\n",
    "response = data[0]['response']\n",
    "trace.span(\n",
    "    name = \"generation\", input={'question': question, 'context': context}, output={'response': response}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reuse the scores previously calculated for the traces in the sample dataset. In development, you would run the UpTrain evaluations for the single trace as it's created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.score(name='context_relevance', value=res[0]['score_context_relevance'])\n",
    "trace.score(name='factual_accuracy', value=res[0]['score_factual_accuracy'])\n",
    "trace.score(name='response_completeness', value=res[0]['score_response_completeness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UpTrain Evals on a single trace in Langfuse](https://langfuse.com/images/cookbook/uptrain-trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production: Add scores to traces in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate a production environment, we will log our sample dataset to Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interaction in data:\n",
    "    trace = langfuse.trace(name = \"uptrain batch\")\n",
    "    trace.span(\n",
    "        name = \"retrieval\",\n",
    "        input={'question': interaction['question']},\n",
    "        output={'context': interaction['context']}\n",
    "    )\n",
    "    trace.span(\n",
    "        name = \"generation\",\n",
    "        input={'question': interaction['question'], 'context': interaction['context']},\n",
    "        output={'response': interaction['response']}\n",
    "    )\n",
    " \n",
    "# await that Langfuse SDK has processed all events before trying to retrieve it in the next step\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now retrieve the traces like regular production data and evaluate them using UpTrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traces(name=None, limit=10000, user_id=None):\n",
    "    all_data = []\n",
    "    page = 1\n",
    " \n",
    "    while True:\n",
    "        response = langfuse.client.trace.list(\n",
    "            name=name, page=page, user_id=user_id, order_by=None\n",
    "        )\n",
    "        if not response.data:\n",
    "            break\n",
    "        page += 1\n",
    "        all_data.extend(response.data)\n",
    "        if len(all_data) > limit:\n",
    "            break\n",
    " \n",
    "    return all_data[:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: create a random sample to reduce evaluation costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    " \n",
    "NUM_TRACES_TO_SAMPLE = 4\n",
    "traces = get_traces(name=\"uptrain batch\")\n",
    "traces_sample = sample(traces, NUM_TRACES_TO_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data into a dataset to be used for evaluation with UpTrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_batch = {\n",
    "    \"question\": [],\n",
    "    \"context\": [],\n",
    "    \"response\": [],\n",
    "    \"trace_id\": [],\n",
    "}\n",
    " \n",
    "for t in traces_sample:\n",
    "    observations = [langfuse.client.observations.get(o) for o in t.observations]\n",
    "    for o in observations:\n",
    "        if o.name == 'retrieval':\n",
    "            question = o.input['question']\n",
    "            context = o.output['context']\n",
    "        if o.name=='generation':\n",
    "            answer = o.output['response']\n",
    "    evaluation_batch['question'].append(question)\n",
    "    evaluation_batch['context'].append(context)\n",
    "    evaluation_batch['response'].append(response)\n",
    "    evaluation_batch['trace_id'].append(t.id)\n",
    "\n",
    "\n",
    "data = [dict(zip(evaluation_batch,t)) for t in zip(*evaluation_batch.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the batch using UpTrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = eval_llm.evaluate(\n",
    "    data = data,\n",
    "    checks = [Evals.CONTEXT_RELEVANCE, Evals.FACTUAL_ACCURACY, Evals.RESPONSE_COMPLETENESS]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `trace_id` back the the dataset as it was omitted in the previous step to be compatible with UpTrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res)\n",
    "\n",
    "# add the langfuse trace_id to the result dataframe\n",
    "df[\"trace_id\"] = [d['trace_id'] for d in data]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the evaluations, we can add them back to the traces in Langfuse as [scores](https://langfuse.com/docs/scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    for metric_name in [\"context_relevance\", \"factual_accuracy\",\"response_completeness\"]:\n",
    "        langfuse.score(\n",
    "            name=metric_name,\n",
    "            value=row[\"score_\"+metric_name],\n",
    "            trace_id=row[\"trace_id\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Langfuse, you can now see the scores for each trace and monitor them over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UpTrain Evals on a list of traces in Langfuse](https://langfuse.com/images/cookbook/uptrain-batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
