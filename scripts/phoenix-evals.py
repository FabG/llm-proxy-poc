from time import sleep
from urllib.request import urlopen

from phoenix.trace.trace_dataset import TraceDataset
from phoenix.trace.utils import json_lines_to_df
from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents
from phoenix.evals import OpenAIModel, run_evals
from phoenix.trace import DocumentEvaluations, SpanEvaluations

import nest_asyncio
nest_asyncio.apply()  # needed for concurrent evals in notebook environments

import phoenix as px
import ssl

# To address temporarily CERTIFICATE_VERIFY_FAILED
ssl._create_default_https_context = ssl._create_unverified_context

### Step 1. Export Trace Data and Launch Phoenix
# For the sake of this guide, we'll download some pre-existing trace data collected from a LlamaIndex application
trace_data_url="https://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/context-retrieval/trace.jsonl"

# Replace with the URL to your trace data
with urlopen(trace_data_url) as response:
    lines = [line.decode("utf-8") for line in response.readlines()]
trace_df = json_lines_to_df(lines)

# Constructs a TraceDataset from a dataframe of spans
trace_ds = TraceDataset(trace_df)

# To note this trace data dates back 12/11/2023 around 12:57PM - make sure to select "All Time" in the webapp to see it
session = px.launch_app(trace=trace_ds)
print(f"üî•üê¶ Select All Time in the Phoenix session")

### Step 2. Evaluate and Log Results
# TO DO Address errors from the below command - these are not erroring in a jupyter notebook
#retrieved_documents_df = get_retrieved_documents(px.Client())
#queries_df = get_qa_with_reference(px.Client())

# Phoenix evaluates your application data by prompting an LLM to classify whether a retrieved document is relevant or irrelevant to the corresponding query,
# whether a response is grounded in a retrieved document, etc. You can even get explanations generated by the LLM to help you understand the results of your evaluations!
#eval_model = OpenAIModel(model_name="gpt-4-turbo-preview")

# Define your evaluators. Evaluators are built on top of language models and prompt the LLM to assess the quality of responses, the relevance of retrieved documents, etc.,
# and provide a quality signal even in the absence of human-labeled data.
from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
)

#hallucination_evaluator = HallucinationEvaluator(eval_model)
#qa_correctness_evaluator = QAEvaluator(eval_model)
#relevance_evaluator = RelevanceEvaluator(eval_model)

# Run the evaluations
# nest_asyncio.apply()  # needed for concurrency in notebook environments

#hallucination_eval_df, qa_correctness_eval_df = run_evals(
#    dataframe=queries_df,
#    evaluators=[hallucination_evaluator, qa_correctness_evaluator],
#    provide_explanation=True,
#)

#relevance_eval_df = run_evals(
#    dataframe=retrieved_documents_df,
#    evaluators=[relevance_evaluator],
#    provide_explanation=True,
#)[0]


# Log your evaluations to your running Phoenix session.
#px.Client().log_evaluations(
#    SpanEvaluations(eval_name="Hallucination", dataframe=hallucination_eval_df),
#    SpanEvaluations(eval_name="QA Correctness", dataframe=qa_correctness_eval_df),
#    DocumentEvaluations(eval_name="Relevance", dataframe=relevance_eval_df),
#)


sleep(240)
px.close_app()